import torch
import random
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from utils import calculate_psnr, calculate_ssim, seed_everything, print_gpu_info
from model import get_model
from datetime import datetime
import os
import glob
import argparse

"""
Explanation of Outputs and Images
================================

1. test_results_finetune_noise{noise_std}.png
--------------------------------------------
- Generated By: This script (test_finetune.py), called from main.py.
- Content:
  - Displays num_samples (default 4) test images in a num_samples x 2 grid:
    - Column 1: Pseudo-Clean (Y, BUSI images, treated as pseudo-clean).
    - Column 2: Denoised (Y_hat = f(Y), model output, approximates X).
- Appearance:
  - Pseudo-Clean: Moderately grainy, anatomical details visible but noisy.
  - Denoised: Smoother, reduced speckle, clearer structures (e.g., lesion boundaries).
- Interpretation:
  - Good Result:
    - Denoised: Significant noise reduction compared to Pseudo-Clean, sharp lesion boundaries, preserved textures. PSNR ~34â€“35 dB.
    - Denoised image is clearer than Pseudo-Clean, approaching the unknown clean X.
  - Poor Result:
    - Denoised remains noisy (PSNR <32 dB).
    - Blurring or loss of anatomical details (e.g., lesions indistinct).
    - Artifacts like streaks or unnatural textures.
  - Compare to PSNR: PSNR >34 dB indicates effective denoising, but visual quality (sharpness, detail) is critical.

2. finetune_metrics_noise{noise_std}.png
---------------------------------------
- Generated By: finetune.py.
- Content:
  - Two subplots:
    - Left: MSE Loss per Epoch (Train and Validation).
    - Right: PSNR per Epoch (Train and Validation).
- Appearance:
  - Loss Plot: Decreasing curves (e.g., from 0.019 to 0.007) for both train and val.
  - PSNR Plot: Increasing curves (e.g., from 31 dB to 34.5 dB).
- Interpretation:
  - Good Result:
    - Loss decreases steadily, converges to <0.01.
    - PSNR increases, stabilizes at >34 dB.
    - Train and val curves are close (no overfitting).
  - Poor Result:
    - Loss plateaus early or diverges.
    - PSNR remains low (<32 dB).
    - Large gap between train and val (overfitting).

Interpreting Results
===================

Quantitative Metrics
-------------------
- Pretrain:
  - Loss ~0.0100, PSNR ~33.00 dB: Indicates good Noise2Void reconstruction.
  - Good: PSNR >32 dB, loss <0.01.
  - Poor: PSNR <30 dB (model didnâ€™t learn to fill patches).
- Finetune:
  - Denoised (Y_hat vs. Y): Loss ~0.0075, PSNR ~34.80 dB. Shows effective denoising of pseudo-clean images.
  - Good: PSNR >34 dB, loss <0.01.
  - Poor: PSNR <32 dB (insufficient denoising).
- Comparison:
  - Fine-tuning PSNR should be higher than pretraining, reflecting improved denoising.
  - Since X (clean image) is unavailable, PSNR is computed against Y, so focus on visual improvement.

Qualitative Visuals
-------------------
- Pretrain (test_results_pretrain.png):
  - Good: Reconstructed patches match surrounding textures, preserving lesions and tissues.
  - Poor: Blurry patches, mismatched textures, or lost details.
  - Action if Poor: Increase pretrain_epochs, adjust mask_ratio (e.g., 0.2).
- Finetune (test_results_finetune_noise{noise_std}.png):
  - Good:
    - Denoised: Reduced speckle compared to Pseudo-Clean, clear anatomical structures.
    - Denoised closer to the unknown clean X than Pseudo-Clean.
  - Poor:
    - Persistent noise or blurring.
    - Loss of critical details (e.g., lesion edges).
  - Action if Poor:
    - Adjust noise_std (e.g., 0.05 or 0.2).
    - Increase finetune_epochs or lower finetune_lr.
    - Add perceptual loss for sharper outputs.
"""

def test_finetune(model, test_loader, config, num_samples, random_seed=None):
    """
    Test the finetuned Noisier2Noise model.
    - Input: Y (pseudo-clean, BUSI images).
    - Output: Y_hat (f(Y), denoised image, approximates the unknown clean X).
    - Metrics: PSNR/SSIM between Y_hat and Y (proxy for X, as X is unavailable).
    """
    # Set random seed if provided
    if random_seed is not None:
        seed_everything(random_seed)

    model.eval()
    test_loss = 0
    test_psnr = 0
    test_ssim = 0

    loss_fn = torch.nn.MSELoss()
    with torch.no_grad():
        loop = tqdm(test_loader, desc="Testing Finetuned")
        for _, input in loop:  # Ignore doubly_noisy, use input (Y)
            input = input.to(config.device)

            output = model(input)  # Y_hat = f(Y)
            loss = loss_fn(output, input)
            test_loss += loss.item()
            test_psnr += calculate_psnr(loss).item()
            test_ssim += calculate_ssim(output, input).item()

    avg_test_loss = test_loss / len(test_loader)
    avg_test_psnr = test_psnr / len(test_loader)
    avg_test_ssim = test_ssim / len(test_loader)

    print(f"ðŸ“Š Finetuned Test Results (Y_hat vs. Y, Noise Std={config.noise_std}):")
    print(f"Average Test Loss: {avg_test_loss:.4f}")
    print(f"Average Test PSNR: {avg_test_psnr:.2f} dB")
    print(f"Average Test SSIM: {avg_test_ssim:.4f}")

    # Select random samples for visualization
    dataset = test_loader.dataset
    sample_indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))
    sample_images = []
    with torch.no_grad():
        for idx in sample_indices:
            _, input = dataset[idx]  # Get pseudo-clean image (Y)
            input = input.unsqueeze(0).to(config.device)  # Add batch dimension
            output = model(input)  # Y_hat
            sample_images.append((output[0], input[0]))

    # Visualize sample images
    plt.figure(figsize=(8, 2 * num_samples))
    for i, (output, input) in enumerate(sample_images):
        plt.subplot(num_samples, 2, i*2 + 1)
        plt.imshow(input.cpu().squeeze(), cmap='gray')
        plt.title("Pseudo-Clean")
        plt.axis('off')

        plt.subplot(num_samples, 2, i*2 + 2)
        plt.imshow(output.cpu().squeeze(), cmap='gray')
        plt.title("Denoised")
        plt.axis('off')

    plt.tight_layout()
    save_path = os.path.join(config.output_dir, f"test_results_finetune_noise{config.noise_std}.png")
    plt.savefig(save_path)
    plt.show()
    print(f"ðŸ“¸ Saved visualization to {save_path}")

def main():
    from config import Config
    from dataset import get_dataloaders

    parser = argparse.ArgumentParser(description="Test fine-tuned model for ultrasound denoising")
    parser.add_argument('--random_seed', type=int, default=None,
                        help="Random seed for sample selection (default: None for true randomness)")
    parser.add_argument('--num_samples', type=int, default=None,
                        help="Number of sample images to visualize (default: config.num_samples)")
    args = parser.parse_args()

    print(f"ðŸ•’ Run started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print_gpu_info()

    config = Config()
    # Use num_samples from args if provided, else from config
    num_samples = args.num_samples if args.num_samples is not None else config.num_samples

    # Create timestamped output directory for standalone execution
    if not config._timestamp:
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        config.output_dir = os.path.join("./outs", timestamp)
        config._timestamp = timestamp
    os.makedirs(config.output_dir, exist_ok=True)

    _, _, test_loader = get_dataloaders(config, mode='finetune')
    model = get_model(model_name="resnet", pretrained=False).to(config.device)

    # Look for the latest finetuned checkpoint
    checkpoint_pattern = os.path.join(config.checkpoint_dir, f"finetuned_resnet_noise{config.noise_std}_final_*.pth")
    checkpoint_files = glob.glob(checkpoint_pattern)
    if checkpoint_files:
        checkpoint_path = max(checkpoint_files, key=os.path.getmtime)
        model.load_state_dict(torch.load(checkpoint_path, map_location=config.device))
        print(f"âœ… Loaded weights from {checkpoint_path}")
    else:
        raise FileNotFoundError(f"No finetuned checkpoint found for noise_std={config.noise_std} in {config.checkpoint_dir}")

    print(f"ðŸ§ª Testing on {len(test_loader.dataset)} test images")
    test_finetune(model, test_loader, config, num_samples, random_seed=args.random_seed)

if __name__ == "__main__":
    main()